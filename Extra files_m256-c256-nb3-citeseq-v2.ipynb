{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Because the datasets are SO large (especially the Multiome dataset), instead of running both parts of the project in one notebook (and risk Kaggle running out of storage space then resetting all progress), it is more convenient to separate the multiome and citeseq parts of the project, then later merge the predicted outputs from the two parts together.","metadata":{}},{"cell_type":"markdown","source":"This notebook concerns itself with the CITEseq portion.","metadata":{}},{"cell_type":"markdown","source":"# First, all the basic imports and file names which may or may not be used is loaded in essentially as a header","metadata":{}},{"cell_type":"code","source":"! pip install tables","metadata":{"execution":{"iopub.status.busy":"2022-11-13T22:03:20.224047Z","iopub.execute_input":"2022-11-13T22:03:20.224756Z","iopub.status.idle":"2022-11-13T22:03:31.216658Z","shell.execute_reply.started":"2022-11-13T22:03:20.224712Z","shell.execute_reply":"2022-11-13T22:03:31.214988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os, gc, pickle, datetime, scipy.sparse\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom colorama import Fore, Back, Style\n\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import TruncatedSVD,PCA\nfrom sklearn.metrics import mean_squared_error\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nimport seaborn as sns\nfrom cycler import cycler\nfrom IPython.display import display\n\nimport scipy.sparse","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-13T22:03:31.218804Z","iopub.execute_input":"2022-11-13T22:03:31.219186Z","iopub.status.idle":"2022-11-13T22:03:31.226727Z","shell.execute_reply.started":"2022-11-13T22:03:31.219146Z","shell.execute_reply":"2022-11-13T22:03:31.225731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Directory of the data\nDATA_DIR = \"/kaggle/input/open-problems-multimodal/\"\nFP_CELL_METADATA = os.path.join(DATA_DIR,\"metadata.csv\")\n\nFP_CITE_TRAIN_INPUTS = os.path.join(DATA_DIR,\"train_cite_inputs.h5\")\nFP_CITE_TRAIN_TARGETS = os.path.join(DATA_DIR,\"train_cite_targets.h5\")\nFP_CITE_TEST_INPUTS = os.path.join(DATA_DIR,\"test_cite_inputs.h5\")\n\nFP_MULT_TRAIN_INPUTS = os.path.join(DATA_DIR,\"train_multi_inputs.h5\")\nFP_MULT_TRAIN_TARGETS = os.path.join(DATA_DIR,\"train_multi_targets.h5\")\nFP_MULT_TEST_INPUTS = os.path.join(DATA_DIR,\"test_multi_inputs.h5\")\n\nFP_MULT_TRAIN_TARGETS_idx = \"../input/multimodal-single-cell-as-sparse-matrix/train_multi_targets_idxcol.npz\"\nFP_MULT_TRAIN_TARGETS_sparse = \"../input/multimodal-single-cell-as-sparse-matrix/train_multi_targets_values.sparse.npz\"\nFP_MULT_TRAIN_INPUTS_idx = \"../input/multimodal-single-cell-as-sparse-matrix/train_multi_inputs_idxcol.npz\"\nFP_MULT_TRAIN_INPUTS_sparse = \"../input/multimodal-single-cell-as-sparse-matrix/train_multi_inputs_values.sparse.npz\"\nFP_MULT_TEST_INPUTS_idx = \"../input/multimodal-single-cell-as-sparse-matrix/test_multi_inputs_idxcol.npz\"\nFP_MULT_TEST_INPUTS_sparse = \"../input/multimodal-single-cell-as-sparse-matrix/test_multi_inputs_values.sparse.npz\"\n\nFP_SUBMISSION = os.path.join(DATA_DIR,\"sample_submission.csv\")\nFP_EVALUATION_IDS = os.path.join(DATA_DIR,\"evaluation_ids.csv\")\n\nFP_EVALUATION_IDS_parquet = \"../input/multimodal-single-cell-as-sparse-matrix/evaluation.parquet\"\n\nmulti_ome_only_file = '../input/n256-nb2-multiome/multiome_only_256.csv'","metadata":{"execution":{"iopub.status.busy":"2022-11-13T22:03:31.228392Z","iopub.execute_input":"2022-11-13T22:03:31.228794Z","iopub.status.idle":"2022-11-13T22:03:31.242928Z","shell.execute_reply.started":"2022-11-13T22:03:31.228754Z","shell.execute_reply":"2022-11-13T22:03:31.241733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CITEseq Part: Predicting protein levels","metadata":{}},{"cell_type":"markdown","source":"Now the CITEseq portion begins\n\n\nCode from pourchot: https://www.kaggle.com/code/pourchot/all-in-one-citeseq-multiome-with-keras","metadata":{}},{"cell_type":"code","source":"svd_ncount = 256 # amount of dimensions to keep for SVD later","metadata":{"execution":{"iopub.status.busy":"2022-11-13T22:03:31.244178Z","iopub.execute_input":"2022-11-13T22:03:31.244572Z","iopub.status.idle":"2022-11-13T22:03:31.259232Z","shell.execute_reply.started":"2022-11-13T22:03:31.244535Z","shell.execute_reply":"2022-11-13T22:03:31.257158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load in the data","metadata":{}},{"cell_type":"code","source":"# Load training data\nX = pd.read_hdf(FP_CITE_TRAIN_INPUTS)\nY = pd.read_hdf(FP_CITE_TRAIN_TARGETS)\n\n# Load test inputs\nX_test = pd.read_hdf(FP_CITE_TEST_INPUTS)","metadata":{"execution":{"iopub.status.busy":"2022-11-13T22:03:31.262067Z","iopub.execute_input":"2022-11-13T22:03:31.262507Z","iopub.status.idle":"2022-11-13T22:04:49.003792Z","shell.execute_reply.started":"2022-11-13T22:03:31.262471Z","shell.execute_reply":"2022-11-13T22:04:49.001931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Constant columns (a.k.a. columns that have the same value in all rows) are useless for machine learning. Just like if you are told to differentiate between apples and oranges, and there is a column which indicates whether apples and oranges are fruits and vegetables, both the apples and oranges will be \"fruit,\" which informs you nothing about the difference between apples and oranges.\n\nHence, constant columns found in the training inputs are found in order to be removed from the input data.","metadata":{}},{"cell_type":"code","source":"constant_cols = list(X.columns[(X == 0).all(axis=0).values]) +\\\n                list(X_test.columns[(X_test == 0).all(axis=0).values])\nprint('constant columns ',len(constant_cols))","metadata":{"execution":{"iopub.status.busy":"2022-11-13T22:04:49.006135Z","iopub.execute_input":"2022-11-13T22:04:49.00724Z","iopub.status.idle":"2022-11-13T22:04:50.818824Z","shell.execute_reply.started":"2022-11-13T22:04:49.007185Z","shell.execute_reply":"2022-11-13T22:04:50.81705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove the constant columns from the training data\nX = X.drop(columns = constant_cols)\nXt = X_test.drop(columns = constant_cols)","metadata":{"execution":{"iopub.status.busy":"2022-11-13T22:04:50.820723Z","iopub.execute_input":"2022-11-13T22:04:50.821133Z","iopub.status.idle":"2022-11-13T22:04:53.541568Z","shell.execute_reply.started":"2022-11-13T22:04:50.821078Z","shell.execute_reply":"2022-11-13T22:04:53.539779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The \"important columns\" are columns that appear as training targets. Hence, it is considered important to keep them in mind","metadata":{}},{"cell_type":"code","source":"important_cols = []\nfor y_col in Y.columns:\n    important_cols += [x_col for x_col in X.columns if y_col in x_col]\nprint('important columns ',len(important_cols))","metadata":{"execution":{"iopub.status.busy":"2022-11-13T22:04:53.547407Z","iopub.execute_input":"2022-11-13T22:04:53.547757Z","iopub.status.idle":"2022-11-13T22:04:53.947596Z","shell.execute_reply.started":"2022-11-13T22:04:53.547728Z","shell.execute_reply":"2022-11-13T22:04:53.946252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before this point, the training and testing data has been loaded in order to determine the constant columns. The training and testing data will be loaded now as sparse matrices with the constant columns removed and the important columns kept. The purpose of sparse matrices is to efficiently store data with lots of zeros and also speed up the machine learning processes.","metadata":{}},{"cell_type":"code","source":"# First, taking a look at X shows there are a LOT of zeros:\nX.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-13T22:04:53.949583Z","iopub.execute_input":"2022-11-13T22:04:53.949986Z","iopub.status.idle":"2022-11-13T22:04:53.980113Z","shell.execute_reply.started":"2022-11-13T22:04:53.949949Z","shell.execute_reply":"2022-11-13T22:04:53.979127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# first delete the X, X_test, Xt, and Y to save space\ndel X\ndel X_test\ndel Xt\ndel Y\n\n# load in the metadata since it'll be modified as well in the next cell\n# (Since X and Y are modified, it is convenient to modify the metadata to match\n# at the same time)\nmetadata_df = pd.read_csv(FP_CELL_METADATA, index_col='cell_id')\nmetadata_df = metadata_df[metadata_df.technology==\"citeseq\"] # focus on citeseq right now\nmetadata_df.shape # show the shape","metadata":{"execution":{"iopub.status.busy":"2022-11-13T22:04:53.981636Z","iopub.execute_input":"2022-11-13T22:04:53.981936Z","iopub.status.idle":"2022-11-13T22:04:54.49477Z","shell.execute_reply.started":"2022-11-13T22:04:53.98191Z","shell.execute_reply":"2022-11-13T22:04:54.49379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# 2min 17s\n\n# Now, the data will be converted into sparse matrices\n# (See MSCI CITEseq Keras Quickstart by AMBROSM)\n\n# Read train and convert to sparse matrix\nX = pd.read_hdf(FP_CITE_TRAIN_INPUTS).drop(columns=constant_cols)\ncell_index = X.index\nmeta = metadata_df.reindex(cell_index)\nX0 = X[important_cols].values\nprint(f\"Original X shape: {str(X.shape):14} {X.size*4/1024/1024/1024:2.3f} GByte\")\ngc.collect()\nX = scipy.sparse.csr_matrix(X.values)\ngc.collect()\n\n# Read test and convert to sparse matrix\nXt = pd.read_hdf(FP_CITE_TEST_INPUTS).drop(columns=constant_cols)\ncell_index_test = Xt.index\nmeta_test = metadata_df.reindex(cell_index_test)\nX0t = Xt[important_cols].values\nprint(f\"Original Xt shape: {str(Xt.shape):14} {Xt.size*4/1024/1024/1024:2.3f} GByte\")\ngc.collect()\nXt = scipy.sparse.csr_matrix(Xt.values)","metadata":{"execution":{"iopub.status.busy":"2022-11-13T22:04:54.496319Z","iopub.execute_input":"2022-11-13T22:04:54.496678Z","iopub.status.idle":"2022-11-13T22:07:05.909113Z","shell.execute_reply.started":"2022-11-13T22:04:54.496643Z","shell.execute_reply":"2022-11-13T22:07:05.907822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Perform SVD\nNow perform SVD in order to reduce the number of features","metadata":{}},{"cell_type":"code","source":"%%time\n# 5-6 minutes\n\n# Apply the singular value decomposition\nboth = scipy.sparse.vstack([X, Xt])\nassert both.shape[0] == 119651\nprint(f\"Shape of both before SVD: {both.shape}\")\nsvd = TruncatedSVD(n_components=svd_ncount, random_state=1) # 512 is possible\nboth = svd.fit_transform(both)\nprint(f\"Shape of both after SVD:  {both.shape}\")\n    \n# Hstack the svd output with the important features\nX = both[:70988]\nXt = both[70988:]\ndel both\nX = np.hstack([X, X0])\nXt = np.hstack([Xt, X0t])\nprint(f\"Reduced X shape:  {str(X.shape):14} {X.size*4/1024/1024/1024:2.3f} GByte\")\nprint(f\"Reduced Xt shape: {str(Xt.shape):14} {Xt.size*4/1024/1024/1024:2.3f} GByte\")","metadata":{"execution":{"iopub.status.busy":"2022-11-13T22:07:05.911028Z","iopub.execute_input":"2022-11-13T22:07:05.91148Z","iopub.status.idle":"2022-11-13T22:11:59.001162Z","shell.execute_reply.started":"2022-11-13T22:07:05.911443Z","shell.execute_reply":"2022-11-13T22:11:58.999077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Explained variance:\")\nprint(svd.explained_variance_ratio_.sum())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read Y\nY = pd.read_hdf(FP_CITE_TRAIN_TARGETS)\ny_columns = list(Y.columns)\nY = Y.values\n\n# Normalize the targets row-wise: This doesn't change the correlations,\n# and negative_correlation_loss depends on it\nY -= Y.mean(axis=1).reshape(-1, 1)\nY /= Y.std(axis=1).reshape(-1, 1)\n    \nprint(f\"Y shape: {str(Y.shape):14} {Y.size*4/1024/1024/1024:2.3f} GByte\")","metadata":{"execution":{"iopub.status.busy":"2022-11-13T22:11:59.003775Z","iopub.execute_input":"2022-11-13T22:11:59.004277Z","iopub.status.idle":"2022-11-13T22:11:59.645Z","shell.execute_reply.started":"2022-11-13T22:11:59.004231Z","shell.execute_reply":"2022-11-13T22:11:59.643655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CITEseq learning model\n\nFrom: https://www.kaggle.com/code/pourchot/all-in-one-citeseq-multiome-with-keras/notebook","metadata":{}},{"cell_type":"code","source":"import math\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping\nfrom tensorflow.keras.layers import Dense, Input, Concatenate, Dropout, BatchNormalization","metadata":{"execution":{"iopub.status.busy":"2022-11-13T22:11:59.646557Z","iopub.execute_input":"2022-11-13T22:11:59.646936Z","iopub.status.idle":"2022-11-13T22:11:59.653487Z","shell.execute_reply.started":"2022-11-13T22:11:59.6469Z","shell.execute_reply":"2022-11-13T22:11:59.652368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"metric and loss function from MSCI CITEseq Keras Quickstart by AMBROSM\n\n","metadata":{}},{"cell_type":"code","source":"def correlation_score(y_true, y_pred):\n    \"\"\"Scores the predictions according to the competition rules. \n    \n    It is assumed that the predictions are not constant.\n    \n    Returns the average of each sample's Pearson correlation coefficient\"\"\"\n    if type(y_true) == pd.DataFrame: y_true = y_true.values\n    if type(y_pred) == pd.DataFrame: y_pred = y_pred.values\n    corrsum = 0\n    for i in range(len(y_true)):\n        corrsum += np.corrcoef(y_true[i], y_pred[i])[1, 0]\n    return corrsum / len(y_true)\n\ndef negative_correlation_loss(y_true, y_pred):\n    \"\"\"Negative correlation loss function for Keras\n    \n    Precondition:\n    y_true.mean(axis=1) == 0\n    y_true.std(axis=1) == 1\n    \n    Returns:\n    -1 = perfect positive correlation\n    1 = totally negative correlation\n    \"\"\"\n    my = K.mean(tf.convert_to_tensor(y_pred), axis=1)\n    my = tf.tile(tf.expand_dims(my, axis=1), (1, y_true.shape[1]))\n    ym = y_pred - my\n    r_num = K.sum(tf.multiply(y_true, ym), axis=1)\n    r_den = tf.sqrt(K.sum(K.square(ym), axis=1) * float(y_true.shape[-1]))\n    r = tf.reduce_mean(r_num / r_den)\n    return - r","metadata":{"execution":{"iopub.status.busy":"2022-11-13T22:11:59.655217Z","iopub.execute_input":"2022-11-13T22:11:59.65556Z","iopub.status.idle":"2022-11-13T22:11:59.666964Z","shell.execute_reply.started":"2022-11-13T22:11:59.655529Z","shell.execute_reply":"2022-11-13T22:11:59.665839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LR_START = 0.01\nBATCH_SIZE = 512\n\ndef create_model():\n    \n    reg1 = 9.613e-06\n    reg2 = 1e-07\n    REG1 = tf.keras.regularizers.l2(reg1)\n    REG2 = tf.keras.regularizers.l2(reg2)\n    DROP = 0.1\n\n    activation = 'selu'\n    inputs = Input(shape =(X.shape[1],))\n\n    x0 = Dense(256, \n              kernel_regularizer = REG1,\n              activation = activation,\n             )(inputs)\n    x0 = Dropout(DROP)(x0)\n    \n    \n    x1 = Dense(512, \n               kernel_regularizer = REG1,\n               activation = activation,\n             )(x0)\n    x1 = Dropout(DROP)(x1)\n    \n    \n    x2 = Dense(512, \n               kernel_regularizer = REG1,\n               activation = activation,\n             )(x1) \n    x2= Dropout(DROP)(x2)\n    \n    x3 = Dense(Y.shape[1],\n               kernel_regularizer = REG1,\n               activation = activation,\n             )(x2)\n    x3 = Dropout(DROP)(x3)\n\n         \n    x = Concatenate()([\n                x0, \n                x1, \n                x2, \n                x3\n                ])\n    \n    x = Dense(Y.shape[1], \n                kernel_regularizer = REG2,\n                activation='linear',\n                )(x)\n    \n    \n    model = Model(inputs, x)\n    \n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-11-13T22:11:59.668697Z","iopub.execute_input":"2022-11-13T22:11:59.669008Z","iopub.status.idle":"2022-11-13T22:11:59.685841Z","shell.execute_reply.started":"2022-11-13T22:11:59.668979Z","shell.execute_reply":"2022-11-13T22:11:59.684676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# 13 min 44 s\nVERBOSE = 1\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nEPOCHS = 50 \nN_SPLITS = 3\n\npred_train = np.zeros((Y.shape[0],Y.shape[1]))\n\nnp.random.seed(1)\ntf.random.set_seed(1)\nscore_list = []\nkf = GroupKFold(n_splits=N_SPLITS)\nscore_list = []\n\nfor fold, (idx_tr, idx_va) in enumerate(kf.split(X, groups=meta.donor)):\n    start_time = datetime.datetime.now()\n    model = None\n    gc.collect()\n    \n    X_tr = X[idx_tr]\n    y_tr = Y[idx_tr]\n    X_va = X[idx_va]\n    y_va = Y[idx_va]\n\n    lr = ReduceLROnPlateau(\n                    monitor = \"val_loss\",\n                    factor = 0.9, \n                    patience = 4, \n                    verbose = VERBOSE)\n\n    es = EarlyStopping(\n                    monitor = \"val_loss\",\n                    patience = 40, \n                    verbose = VERBOSE,\n                    mode = \"min\", \n                    restore_best_weights = True)\n\n    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n                    filepath = './citeseq',\n                    save_weights_only = True,\n                    monitor = 'val_loss',\n                    mode = 'min',\n                    save_best_only = True)\n\n    callbacks = [\n                    lr, \n                    es, \n                    model_checkpoint_callback\n                    ]\n    \n    model = create_model()\n    \n    model.compile(\n                optimizer = tf.keras.optimizers.Adam(learning_rate=LR_START),\n                metrics = [negative_correlation_loss],\n                loss = negative_correlation_loss\n                 )\n    # Training\n    model.fit(\n                X_tr,\n                y_tr, \n                validation_data=(\n                                X_va,\n                                y_va), \n                epochs = EPOCHS,\n                verbose = VERBOSE,\n                batch_size = BATCH_SIZE,\n                shuffle = True,\n                callbacks = callbacks)\n\n    del X_tr, y_tr \n    gc.collect()\n    \n    model.load_weights('./citeseq')\n    model.save(f\"./submissions/model_{fold}\")\n    print('model saved')\n    \n    #  Model validation\n    y_va_pred = model.predict(X_va)\n    corrscore = correlation_score(y_va, y_va_pred)\n    pred_train[idx_va] = y_va_pred\n    \n    print(f\"Fold {fold}, correlation =  {corrscore:.5f}\")\n    del X_va, y_va, y_va_pred\n    gc.collect()\n    score_list.append(corrscore)\n\n# Show overall score\nprint(f\"{Fore.GREEN}{Style.BRIGHT}Mean corr = {np.array(score_list).mean():.5f}{Style.RESET_ALL}\")\nscore_total = correlation_score(Y, pred_train)\nprint(f\"{Fore.BLUE}{Style.BRIGHT}Oof corr   = {score_total:.5f}{Style.RESET_ALL}\")","metadata":{"execution":{"iopub.status.busy":"2022-11-13T22:11:59.687759Z","iopub.execute_input":"2022-11-13T22:11:59.688102Z","iopub.status.idle":"2022-11-13T22:22:23.505481Z","shell.execute_reply.started":"2022-11-13T22:11:59.688052Z","shell.execute_reply":"2022-11-13T22:22:23.503458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predictions for CITEseq","metadata":{}},{"cell_type":"code","source":"%%time\n# Around 20 s\n\ntest_pred = np.zeros((len(Xt), 140), dtype=np.float32)\nfor fold in range(N_SPLITS):\n    print(f\"Predicting with fold {fold}\")\n    model = load_model(f\"./submissions/model_{fold}\",\n                       custom_objects={'negative_correlation_loss': negative_correlation_loss})\n    test_pred += model.predict(Xt)","metadata":{"execution":{"iopub.status.busy":"2022-11-13T22:22:23.50739Z","iopub.execute_input":"2022-11-13T22:22:23.508374Z","iopub.status.idle":"2022-11-13T22:22:38.475416Z","shell.execute_reply.started":"2022-11-13T22:22:23.508332Z","shell.execute_reply":"2022-11-13T22:22:38.473543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save submission by merging with multiome","metadata":{}},{"cell_type":"code","source":"%%time\n# 2min 41s\n\n# Merge with multiome\nsubmission = pd.read_csv(multi_ome_only_file,index_col='row_id', squeeze=True)\nsubmission.iloc[:len(test_pred.ravel())] = test_pred.ravel()\nassert not submission.isna().any()\n\nsubmission.to_csv('submission_full_m256_c256.csv')\ndisplay(submission)","metadata":{"execution":{"iopub.status.busy":"2022-11-13T22:22:38.478116Z","iopub.execute_input":"2022-11-13T22:22:38.478647Z","iopub.status.idle":"2022-11-13T22:25:20.389651Z","shell.execute_reply.started":"2022-11-13T22:22:38.478602Z","shell.execute_reply":"2022-11-13T22:25:20.38829Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Because the datasets are SO large (especially the Multiome dataset), instead of running both parts of the project in one notebook (and risk Kaggle running out of storage space then resetting all progress), it is more convenient to separate the multiome and citeseq parts of the project, then later merge the predicted outputs from the two parts together.","metadata":{}},{"cell_type":"markdown","source":"This notebook concerns itself with the CITEseq portion.","metadata":{}},{"cell_type":"markdown","source":"# First, all the basic imports and file names which may or may not be used is loaded in essentially as a header","metadata":{}},{"cell_type":"code","source":"! pip install tables","metadata":{"execution":{"iopub.status.busy":"2022-11-14T03:52:43.099989Z","iopub.execute_input":"2022-11-14T03:52:43.100511Z","iopub.status.idle":"2022-11-14T03:52:56.184958Z","shell.execute_reply.started":"2022-11-14T03:52:43.100418Z","shell.execute_reply":"2022-11-14T03:52:56.183369Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tables in /opt/conda/lib/python3.7/site-packages (3.7.0)\nRequirement already satisfied: numexpr>=2.6.2 in /opt/conda/lib/python3.7/site-packages (from tables) (2.8.3)\nRequirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.7/site-packages (from tables) (1.21.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tables) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->tables) (3.0.9)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os, gc, pickle, datetime, scipy.sparse\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom colorama import Fore, Back, Style\n\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import TruncatedSVD,PCA\nfrom sklearn.metrics import mean_squared_error\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nimport seaborn as sns\nfrom cycler import cycler\nfrom IPython.display import display\n\nimport scipy.sparse","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-14T03:52:56.187463Z","iopub.execute_input":"2022-11-14T03:52:56.187853Z","iopub.status.idle":"2022-11-14T03:52:57.057740Z","shell.execute_reply.started":"2022-11-14T03:52:56.187815Z","shell.execute_reply":"2022-11-14T03:52:57.056473Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Directory of the data\nDATA_DIR = \"/kaggle/input/open-problems-multimodal/\"\nFP_CELL_METADATA = os.path.join(DATA_DIR,\"metadata.csv\")\n\nFP_CITE_TRAIN_INPUTS = os.path.join(DATA_DIR,\"train_cite_inputs.h5\")\nFP_CITE_TRAIN_TARGETS = os.path.join(DATA_DIR,\"train_cite_targets.h5\")\nFP_CITE_TEST_INPUTS = os.path.join(DATA_DIR,\"test_cite_inputs.h5\")\n\nFP_MULT_TRAIN_INPUTS = os.path.join(DATA_DIR,\"train_multi_inputs.h5\")\nFP_MULT_TRAIN_TARGETS = os.path.join(DATA_DIR,\"train_multi_targets.h5\")\nFP_MULT_TEST_INPUTS = os.path.join(DATA_DIR,\"test_multi_inputs.h5\")\n\nFP_MULT_TRAIN_TARGETS_idx = \"../input/multimodal-single-cell-as-sparse-matrix/train_multi_targets_idxcol.npz\"\nFP_MULT_TRAIN_TARGETS_sparse = \"../input/multimodal-single-cell-as-sparse-matrix/train_multi_targets_values.sparse.npz\"\nFP_MULT_TRAIN_INPUTS_idx = \"../input/multimodal-single-cell-as-sparse-matrix/train_multi_inputs_idxcol.npz\"\nFP_MULT_TRAIN_INPUTS_sparse = \"../input/multimodal-single-cell-as-sparse-matrix/train_multi_inputs_values.sparse.npz\"\nFP_MULT_TEST_INPUTS_idx = \"../input/multimodal-single-cell-as-sparse-matrix/test_multi_inputs_idxcol.npz\"\nFP_MULT_TEST_INPUTS_sparse = \"../input/multimodal-single-cell-as-sparse-matrix/test_multi_inputs_values.sparse.npz\"\n\nFP_SUBMISSION = os.path.join(DATA_DIR,\"sample_submission.csv\")\nFP_EVALUATION_IDS = os.path.join(DATA_DIR,\"evaluation_ids.csv\")\n\nFP_EVALUATION_IDS_parquet = \"../input/multimodal-single-cell-as-sparse-matrix/evaluation.parquet\"\n\nmulti_ome_only_file = '../input/nb2multiome/multiome_only.csv'","metadata":{"execution":{"iopub.status.busy":"2022-11-14T03:52:57.059284Z","iopub.execute_input":"2022-11-14T03:52:57.059686Z","iopub.status.idle":"2022-11-14T03:52:57.068747Z","shell.execute_reply.started":"2022-11-14T03:52:57.059652Z","shell.execute_reply":"2022-11-14T03:52:57.067876Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# CITEseq Part: Predicting protein levels","metadata":{}},{"cell_type":"markdown","source":"Now the CITEseq portion begins\n\n\nCode from pourchot: https://www.kaggle.com/code/pourchot/all-in-one-citeseq-multiome-with-keras","metadata":{}},{"cell_type":"code","source":"svd_ncount = 128 # amount of dimensions to keep for SVD later","metadata":{"execution":{"iopub.status.busy":"2022-11-14T03:52:57.070989Z","iopub.execute_input":"2022-11-14T03:52:57.072265Z","iopub.status.idle":"2022-11-14T03:52:57.084365Z","shell.execute_reply.started":"2022-11-14T03:52:57.072217Z","shell.execute_reply":"2022-11-14T03:52:57.083277Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Load in the data","metadata":{}},{"cell_type":"code","source":"# Load training data\nX = pd.read_hdf(FP_CITE_TRAIN_INPUTS)\nY = pd.read_hdf(FP_CITE_TRAIN_TARGETS)\n\n# Load test inputs\nX_test = pd.read_hdf(FP_CITE_TEST_INPUTS)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T03:52:57.086112Z","iopub.execute_input":"2022-11-14T03:52:57.086841Z","iopub.status.idle":"2022-11-14T03:54:22.700739Z","shell.execute_reply.started":"2022-11-14T03:52:57.086794Z","shell.execute_reply":"2022-11-14T03:54:22.698240Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Constant columns (a.k.a. columns that have the same value in all rows) are useless for machine learning. Just like if you are told to differentiate between apples and oranges, and there is a column which indicates whether apples and oranges are fruits and vegetables, both the apples and oranges will be \"fruit,\" which informs you nothing about the difference between apples and oranges.\n\nHence, constant columns found in the training inputs are found in order to be removed from the input data.","metadata":{}},{"cell_type":"code","source":"constant_cols = list(X.columns[(X == 0).all(axis=0).values]) +\\\n                list(X_test.columns[(X_test == 0).all(axis=0).values])\nprint('constant columns ',len(constant_cols))","metadata":{"execution":{"iopub.status.busy":"2022-11-14T03:54:22.705228Z","iopub.execute_input":"2022-11-14T03:54:22.705969Z","iopub.status.idle":"2022-11-14T03:54:25.331536Z","shell.execute_reply.started":"2022-11-14T03:54:22.705922Z","shell.execute_reply":"2022-11-14T03:54:25.329886Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"constant columns  1194\n","output_type":"stream"}]},{"cell_type":"code","source":"# remove the constant columns from the training data\nX = X.drop(columns = constant_cols)\nXt = X_test.drop(columns = constant_cols)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T03:54:25.333963Z","iopub.execute_input":"2022-11-14T03:54:25.334941Z","iopub.status.idle":"2022-11-14T03:54:28.632196Z","shell.execute_reply.started":"2022-11-14T03:54:25.334886Z","shell.execute_reply":"2022-11-14T03:54:28.631249Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"The \"important columns\" are columns that appear as training targets. Hence, it is considered important to keep them in mind","metadata":{}},{"cell_type":"code","source":"important_cols = []\nfor y_col in Y.columns:\n    important_cols += [x_col for x_col in X.columns if y_col in x_col]\nprint('important columns ',len(important_cols))","metadata":{"execution":{"iopub.status.busy":"2022-11-14T03:54:28.634988Z","iopub.execute_input":"2022-11-14T03:54:28.635396Z","iopub.status.idle":"2022-11-14T03:54:29.090210Z","shell.execute_reply.started":"2022-11-14T03:54:28.635361Z","shell.execute_reply":"2022-11-14T03:54:29.089012Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"important columns  144\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Before this point, the training and testing data has been loaded in order to determine the constant columns. The training and testing data will be loaded now as sparse matrices with the constant columns removed and the important columns kept. The purpose of sparse matrices is to efficiently store data with lots of zeros and also speed up the machine learning processes.","metadata":{}},{"cell_type":"code","source":"# First, taking a look at X shows there are a LOT of zeros:\nX.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-14T03:54:29.091808Z","iopub.execute_input":"2022-11-14T03:54:29.092824Z","iopub.status.idle":"2022-11-14T03:54:29.135624Z","shell.execute_reply.started":"2022-11-14T03:54:29.092788Z","shell.execute_reply":"2022-11-14T03:54:29.134475Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"gene_id       ENSG00000121410_A1BG  ENSG00000268895_A1BG-AS1  \\\ncell_id                                                        \n45006fe3e4c8                   0.0                       0.0   \nd02759a80ba2                   0.0                       0.0   \nc016c6b0efa5                   0.0                       0.0   \nba7f733a4f75                   0.0                       0.0   \nfbcf2443ffb2                   0.0                       0.0   \n\ngene_id       ENSG00000175899_A2M  ENSG00000245105_A2M-AS1  \\\ncell_id                                                      \n45006fe3e4c8                  0.0                      0.0   \nd02759a80ba2                  0.0                      0.0   \nc016c6b0efa5                  0.0                      0.0   \nba7f733a4f75                  0.0                      0.0   \nfbcf2443ffb2                  0.0                      0.0   \n\ngene_id       ENSG00000128274_A4GALT  ENSG00000094914_AAAS  \\\ncell_id                                                      \n45006fe3e4c8                0.000000              0.000000   \nd02759a80ba2                0.000000              0.000000   \nc016c6b0efa5                3.847321              0.000000   \nba7f733a4f75                0.000000              3.436846   \nfbcf2443ffb2                0.000000              0.000000   \n\ngene_id       ENSG00000081760_AACS  ENSG00000109576_AADAT  \\\ncell_id                                                     \n45006fe3e4c8              0.000000               0.000000   \nd02759a80ba2              0.000000               0.000000   \nc016c6b0efa5              3.847321               3.847321   \nba7f733a4f75              3.436846               0.000000   \nfbcf2443ffb2              4.196826               0.000000   \n\ngene_id       ENSG00000103591_AAGAB  ENSG00000115977_AAK1  ...  \\\ncell_id                                                    ...   \n45006fe3e4c8                    0.0              0.000000  ...   \nd02759a80ba2                    0.0              4.039545  ...   \nc016c6b0efa5                    0.0              0.000000  ...   \nba7f733a4f75                    0.0              4.513782  ...   \nfbcf2443ffb2                    0.0              0.000000  ...   \n\ngene_id       ENSG00000153975_ZUP1  ENSG00000086827_ZW10  \\\ncell_id                                                    \n45006fe3e4c8              0.000000              0.000000   \nd02759a80ba2              0.000000              0.000000   \nc016c6b0efa5              0.000000              0.000000   \nba7f733a4f75              3.436846              0.000000   \nfbcf2443ffb2              0.000000              4.196826   \n\ngene_id       ENSG00000174442_ZWILCH  ENSG00000122952_ZWINT  \\\ncell_id                                                       \n45006fe3e4c8                0.000000               0.000000   \nd02759a80ba2                0.000000               4.039545   \nc016c6b0efa5                3.847321               4.529743   \nba7f733a4f75                4.113780               5.020215   \nfbcf2443ffb2                4.196826               4.196826   \n\ngene_id       ENSG00000198205_ZXDA  ENSG00000198455_ZXDB  \\\ncell_id                                                    \n45006fe3e4c8                   0.0                   0.0   \nd02759a80ba2                   0.0                   0.0   \nc016c6b0efa5                   0.0                   0.0   \nba7f733a4f75                   0.0                   0.0   \nfbcf2443ffb2                   0.0                   0.0   \n\ngene_id       ENSG00000070476_ZXDC  ENSG00000162378_ZYG11B  \\\ncell_id                                                      \n45006fe3e4c8               0.00000                0.000000   \nd02759a80ba2               0.00000                0.000000   \nc016c6b0efa5               0.00000                3.847321   \nba7f733a4f75               0.00000                3.436846   \nfbcf2443ffb2               3.51861                4.196826   \n\ngene_id       ENSG00000159840_ZYX  ENSG00000074755_ZZEF1  \ncell_id                                                   \n45006fe3e4c8             4.090185                    0.0  \nd02759a80ba2             0.000000                    0.0  \nc016c6b0efa5             3.847321                    0.0  \nba7f733a4f75             4.113780                    0.0  \nfbcf2443ffb2             3.518610                    0.0  \n\n[5 rows x 20856 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>gene_id</th>\n      <th>ENSG00000121410_A1BG</th>\n      <th>ENSG00000268895_A1BG-AS1</th>\n      <th>ENSG00000175899_A2M</th>\n      <th>ENSG00000245105_A2M-AS1</th>\n      <th>ENSG00000128274_A4GALT</th>\n      <th>ENSG00000094914_AAAS</th>\n      <th>ENSG00000081760_AACS</th>\n      <th>ENSG00000109576_AADAT</th>\n      <th>ENSG00000103591_AAGAB</th>\n      <th>ENSG00000115977_AAK1</th>\n      <th>...</th>\n      <th>ENSG00000153975_ZUP1</th>\n      <th>ENSG00000086827_ZW10</th>\n      <th>ENSG00000174442_ZWILCH</th>\n      <th>ENSG00000122952_ZWINT</th>\n      <th>ENSG00000198205_ZXDA</th>\n      <th>ENSG00000198455_ZXDB</th>\n      <th>ENSG00000070476_ZXDC</th>\n      <th>ENSG00000162378_ZYG11B</th>\n      <th>ENSG00000159840_ZYX</th>\n      <th>ENSG00000074755_ZZEF1</th>\n    </tr>\n    <tr>\n      <th>cell_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>45006fe3e4c8</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>4.090185</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>d02759a80ba2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>4.039545</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>4.039545</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>c016c6b0efa5</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.847321</td>\n      <td>0.000000</td>\n      <td>3.847321</td>\n      <td>3.847321</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>3.847321</td>\n      <td>4.529743</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>3.847321</td>\n      <td>3.847321</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>ba7f733a4f75</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>3.436846</td>\n      <td>3.436846</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>4.513782</td>\n      <td>...</td>\n      <td>3.436846</td>\n      <td>0.000000</td>\n      <td>4.113780</td>\n      <td>5.020215</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.00000</td>\n      <td>3.436846</td>\n      <td>4.113780</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>fbcf2443ffb2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>4.196826</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>4.196826</td>\n      <td>4.196826</td>\n      <td>4.196826</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.51861</td>\n      <td>4.196826</td>\n      <td>3.518610</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 20856 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# first delete the X, X_test, Xt, and Y to save space\ndel X\ndel X_test\ndel Xt\ndel Y\n\n# load in the metadata since it'll be modified as well in the next cell\n# (Since X and Y are modified, it is convenient to modify the metadata to match\n# at the same time)\nmetadata_df = pd.read_csv(FP_CELL_METADATA, index_col='cell_id')\nmetadata_df = metadata_df[metadata_df.technology==\"citeseq\"] # focus on citeseq right now\nmetadata_df.shape # show the shape","metadata":{"execution":{"iopub.status.busy":"2022-11-14T03:54:29.139574Z","iopub.execute_input":"2022-11-14T03:54:29.139951Z","iopub.status.idle":"2022-11-14T03:54:29.693943Z","shell.execute_reply.started":"2022-11-14T03:54:29.139918Z","shell.execute_reply":"2022-11-14T03:54:29.692800Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(119651, 4)"},"metadata":{}}]},{"cell_type":"code","source":"%%time\n# 2min 17s\n\n# Now, the data will be converted into sparse matrices\n# (See MSCI CITEseq Keras Quickstart by AMBROSM)\n\n# Read train and convert to sparse matrix\nX = pd.read_hdf(FP_CITE_TRAIN_INPUTS).drop(columns=constant_cols)\ncell_index = X.index\nmeta = metadata_df.reindex(cell_index)\nX0 = X[important_cols].values\nprint(f\"Original X shape: {str(X.shape):14} {X.size*4/1024/1024/1024:2.3f} GByte\")\ngc.collect()\nX = scipy.sparse.csr_matrix(X.values)\ngc.collect()\n\n# Read test and convert to sparse matrix\nXt = pd.read_hdf(FP_CITE_TEST_INPUTS).drop(columns=constant_cols)\ncell_index_test = Xt.index\nmeta_test = metadata_df.reindex(cell_index_test)\nX0t = Xt[important_cols].values\nprint(f\"Original Xt shape: {str(Xt.shape):14} {Xt.size*4/1024/1024/1024:2.3f} GByte\")\ngc.collect()\nXt = scipy.sparse.csr_matrix(Xt.values)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T03:54:29.695309Z","iopub.execute_input":"2022-11-14T03:54:29.695657Z","iopub.status.idle":"2022-11-14T03:57:16.926708Z","shell.execute_reply.started":"2022-11-14T03:54:29.695627Z","shell.execute_reply":"2022-11-14T03:57:16.925374Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Original X shape: (70988, 20856) 5.515 GByte\nOriginal Xt shape: (48663, 20856) 3.781 GByte\nCPU times: user 2min 2s, sys: 13.6 s, total: 2min 16s\nWall time: 2min 47s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Perform SVD\nNow perform SVD in order to reduce the number of features","metadata":{}},{"cell_type":"code","source":"%%time\n# 5-6 minutes\n\n# Apply the singular value decomposition\nboth = scipy.sparse.vstack([X, Xt])\nassert both.shape[0] == 119651\nprint(f\"Shape of both before SVD: {both.shape}\")\nsvd = TruncatedSVD(n_components=svd_ncount, random_state=1) # 512 is possible\nboth = svd.fit_transform(both)\nprint(f\"Shape of both after SVD:  {both.shape}\")\n    \n# Hstack the svd output with the important features\nX = both[:70988]\nXt = both[70988:]\ndel both\nX = np.hstack([X, X0])\nXt = np.hstack([Xt, X0t])\nprint(f\"Reduced X shape:  {str(X.shape):14} {X.size*4/1024/1024/1024:2.3f} GByte\")\nprint(f\"Reduced Xt shape: {str(Xt.shape):14} {Xt.size*4/1024/1024/1024:2.3f} GByte\")","metadata":{"execution":{"iopub.status.busy":"2022-11-14T03:57:16.929180Z","iopub.execute_input":"2022-11-14T03:57:16.930351Z","iopub.status.idle":"2022-11-14T04:05:54.478656Z","shell.execute_reply.started":"2022-11-14T03:57:16.930281Z","shell.execute_reply":"2022-11-14T04:05:54.477287Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Shape of both before SVD: (119651, 20856)\nShape of both after SVD:  (119651, 128)\nReduced X shape:  (70988, 272)   0.072 GByte\nReduced Xt shape: (48663, 272)   0.049 GByte\nCPU times: user 8min 38s, sys: 7.86 s, total: 8min 46s\nWall time: 8min 37s\n","output_type":"stream"}]},{"cell_type":"code","source":"# Read Y\nY = pd.read_hdf(FP_CITE_TRAIN_TARGETS)\ny_columns = list(Y.columns)\nY = Y.values\n\n# Normalize the targets row-wise: This doesn't change the correlations,\n# and negative_correlation_loss depends on it\nY -= Y.mean(axis=1).reshape(-1, 1)\nY /= Y.std(axis=1).reshape(-1, 1)\n    \nprint(f\"Y shape: {str(Y.shape):14} {Y.size*4/1024/1024/1024:2.3f} GByte\")","metadata":{"execution":{"iopub.status.busy":"2022-11-14T04:05:54.480636Z","iopub.execute_input":"2022-11-14T04:05:54.481073Z","iopub.status.idle":"2022-11-14T04:05:55.185971Z","shell.execute_reply.started":"2022-11-14T04:05:54.480983Z","shell.execute_reply":"2022-11-14T04:05:55.184477Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Y shape: (70988, 140)   0.037 GByte\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## CITEseq learning model\n\nFrom: https://www.kaggle.com/code/pourchot/all-in-one-citeseq-multiome-with-keras/notebook","metadata":{}},{"cell_type":"code","source":"import math\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping\nfrom tensorflow.keras.layers import Dense, Input, Concatenate, Dropout, BatchNormalization","metadata":{"execution":{"iopub.status.busy":"2022-11-14T04:05:55.187541Z","iopub.execute_input":"2022-11-14T04:05:55.188384Z","iopub.status.idle":"2022-11-14T04:06:02.229528Z","shell.execute_reply.started":"2022-11-14T04:05:55.188346Z","shell.execute_reply":"2022-11-14T04:06:02.228202Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"metric and loss function from MSCI CITEseq Keras Quickstart by AMBROSM\n\n","metadata":{}},{"cell_type":"code","source":"def correlation_score(y_true, y_pred):\n    \"\"\"Scores the predictions according to the competition rules. \n    \n    It is assumed that the predictions are not constant.\n    \n    Returns the average of each sample's Pearson correlation coefficient\"\"\"\n    if type(y_true) == pd.DataFrame: y_true = y_true.values\n    if type(y_pred) == pd.DataFrame: y_pred = y_pred.values\n    corrsum = 0\n    for i in range(len(y_true)):\n        corrsum += np.corrcoef(y_true[i], y_pred[i])[1, 0]\n    return corrsum / len(y_true)\n\ndef negative_correlation_loss(y_true, y_pred):\n    \"\"\"Negative correlation loss function for Keras\n    \n    Precondition:\n    y_true.mean(axis=1) == 0\n    y_true.std(axis=1) == 1\n    \n    Returns:\n    -1 = perfect positive correlation\n    1 = totally negative correlation\n    \"\"\"\n    my = K.mean(tf.convert_to_tensor(y_pred), axis=1)\n    my = tf.tile(tf.expand_dims(my, axis=1), (1, y_true.shape[1]))\n    ym = y_pred - my\n    r_num = K.sum(tf.multiply(y_true, ym), axis=1)\n    r_den = tf.sqrt(K.sum(K.square(ym), axis=1) * float(y_true.shape[-1]))\n    r = tf.reduce_mean(r_num / r_den)\n    return - r","metadata":{"execution":{"iopub.status.busy":"2022-11-14T04:06:02.231377Z","iopub.execute_input":"2022-11-14T04:06:02.232120Z","iopub.status.idle":"2022-11-14T04:06:02.246138Z","shell.execute_reply.started":"2022-11-14T04:06:02.232078Z","shell.execute_reply":"2022-11-14T04:06:02.244805Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"LR_START = 0.01\nBATCH_SIZE = 512\n\ndef create_model():\n    \n    reg1 = 9.613e-06\n    reg2 = 1e-07\n    REG1 = tf.keras.regularizers.l2(reg1)\n    REG2 = tf.keras.regularizers.l2(reg2)\n    DROP = 0.1\n\n    activation = 'selu'\n    inputs = Input(shape =(X.shape[1],))\n\n    x0 = Dense(256, \n              kernel_regularizer = REG1,\n              activation = activation,\n             )(inputs)\n    x0 = Dropout(DROP)(x0)\n    \n    \n    x1 = Dense(512, \n               kernel_regularizer = REG1,\n               activation = activation,\n             )(x0)\n    x1 = Dropout(DROP)(x1)\n    \n    \n    x2 = Dense(512, \n               kernel_regularizer = REG1,\n               activation = activation,\n             )(x1) \n    x2= Dropout(DROP)(x2)\n    \n    x3 = Dense(Y.shape[1],\n               kernel_regularizer = REG1,\n               activation = activation,\n             )(x2)\n    x3 = Dropout(DROP)(x3)\n\n         \n    x = Concatenate()([\n                x0, \n                x1, \n                x2, \n                x3\n                ])\n    \n    x = Dense(Y.shape[1], \n                kernel_regularizer = REG2,\n                activation='linear',\n                )(x)\n    \n    \n    model = Model(inputs, x)\n    \n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-11-14T04:06:02.249172Z","iopub.execute_input":"2022-11-14T04:06:02.250356Z","iopub.status.idle":"2022-11-14T04:06:02.282980Z","shell.execute_reply.started":"2022-11-14T04:06:02.250313Z","shell.execute_reply":"2022-11-14T04:06:02.281656Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"%%time\n# 13 min 44 s\nVERBOSE = 1\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nEPOCHS = 50 \nN_SPLITS = 3\n\npred_train = np.zeros((Y.shape[0],Y.shape[1]))\n\nnp.random.seed(1)\ntf.random.set_seed(1)\nscore_list = []\nkf = GroupKFold(n_splits=N_SPLITS)\nscore_list = []\n\nfor fold, (idx_tr, idx_va) in enumerate(kf.split(X, groups=meta.donor)):\n    start_time = datetime.datetime.now()\n    model = None\n    gc.collect()\n    \n    X_tr = X[idx_tr]\n    y_tr = Y[idx_tr]\n    X_va = X[idx_va]\n    y_va = Y[idx_va]\n\n    lr = ReduceLROnPlateau(\n                    monitor = \"val_loss\",\n                    factor = 0.9, \n                    patience = 4, \n                    verbose = VERBOSE)\n\n    es = EarlyStopping(\n                    monitor = \"val_loss\",\n                    patience = 40, \n                    verbose = VERBOSE,\n                    mode = \"min\", \n                    restore_best_weights = True)\n\n    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n                    filepath = './citeseq',\n                    save_weights_only = True,\n                    monitor = 'val_loss',\n                    mode = 'min',\n                    save_best_only = True)\n\n    callbacks = [\n                    lr, \n                    es, \n                    model_checkpoint_callback\n                    ]\n    \n    model = create_model()\n    \n    model.compile(\n                optimizer = tf.keras.optimizers.Adam(learning_rate=LR_START),\n                metrics = [negative_correlation_loss],\n                loss = negative_correlation_loss\n                 )\n    # Training\n    model.fit(\n                X_tr,\n                y_tr, \n                validation_data=(\n                                X_va,\n                                y_va), \n                epochs = EPOCHS,\n                verbose = VERBOSE,\n                batch_size = BATCH_SIZE,\n                shuffle = True,\n                callbacks = callbacks)\n\n    del X_tr, y_tr \n    gc.collect()\n    \n    model.load_weights('./citeseq')\n    model.save(f\"./submissions/model_{fold}\")\n    print('model saved')\n    \n    #  Model validation\n    y_va_pred = model.predict(X_va)\n    corrscore = correlation_score(y_va, y_va_pred)\n    pred_train[idx_va] = y_va_pred\n    \n    print(f\"Fold {fold}, correlation =  {corrscore:.5f}\")\n    del X_va, y_va, y_va_pred\n    gc.collect()\n    score_list.append(corrscore)\n\n# Show overall score\nprint(f\"{Fore.GREEN}{Style.BRIGHT}Mean corr = {np.array(score_list).mean():.5f}{Style.RESET_ALL}\")\nscore_total = correlation_score(Y, pred_train)\nprint(f\"{Fore.BLUE}{Style.BRIGHT}Oof corr   = {score_total:.5f}{Style.RESET_ALL}\")","metadata":{"execution":{"iopub.status.busy":"2022-11-14T04:06:02.284843Z","iopub.execute_input":"2022-11-14T04:06:02.286125Z","iopub.status.idle":"2022-11-14T04:18:00.003870Z","shell.execute_reply.started":"2022-11-14T04:06:02.286037Z","shell.execute_reply":"2022-11-14T04:18:00.002517Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"2022-11-14 04:06:02.636647: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n2022-11-14 04:06:03.003144: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n91/91 [==============================] - 6s 52ms/step - loss: -0.8213 - negative_correlation_loss: -0.8381 - val_loss: -0.8531 - val_negative_correlation_loss: -0.8661\nEpoch 2/50\n91/91 [==============================] - 4s 48ms/step - loss: -0.8740 - negative_correlation_loss: -0.8849 - val_loss: -0.8682 - val_negative_correlation_loss: -0.8768\nEpoch 3/50\n91/91 [==============================] - 4s 48ms/step - loss: -0.8853 - negative_correlation_loss: -0.8930 - val_loss: -0.8721 - val_negative_correlation_loss: -0.8785\nEpoch 4/50\n91/91 [==============================] - 4s 48ms/step - loss: -0.8899 - negative_correlation_loss: -0.8962 - val_loss: -0.8787 - val_negative_correlation_loss: -0.8844\nEpoch 5/50\n91/91 [==============================] - 4s 47ms/step - loss: -0.8930 - negative_correlation_loss: -0.8986 - val_loss: -0.8790 - val_negative_correlation_loss: -0.8841\nEpoch 6/50\n91/91 [==============================] - 4s 48ms/step - loss: -0.8947 - negative_correlation_loss: -0.8999 - val_loss: -0.8814 - val_negative_correlation_loss: -0.8861\nEpoch 7/50\n91/91 [==============================] - 5s 56ms/step - loss: -0.8954 - negative_correlation_loss: -0.9004 - val_loss: -0.8808 - val_negative_correlation_loss: -0.8853\nEpoch 8/50\n91/91 [==============================] - 4s 48ms/step - loss: -0.8962 - negative_correlation_loss: -0.9009 - val_loss: -0.8820 - val_negative_correlation_loss: -0.8863\nEpoch 9/50\n91/91 [==============================] - 4s 49ms/step - loss: -0.8968 - negative_correlation_loss: -0.9013 - val_loss: -0.8826 - val_negative_correlation_loss: -0.8868\nEpoch 10/50\n91/91 [==============================] - 5s 50ms/step - loss: -0.8972 - negative_correlation_loss: -0.9017 - val_loss: -0.8840 - val_negative_correlation_loss: -0.8881\nEpoch 11/50\n91/91 [==============================] - 4s 49ms/step - loss: -0.8975 - negative_correlation_loss: -0.9019 - val_loss: -0.8825 - val_negative_correlation_loss: -0.8866\nEpoch 12/50\n91/91 [==============================] - 4s 48ms/step - loss: -0.8978 - negative_correlation_loss: -0.9019 - val_loss: -0.8840 - val_negative_correlation_loss: -0.8879\nEpoch 13/50\n91/91 [==============================] - 4s 47ms/step - loss: -0.8977 - negative_correlation_loss: -0.9020 - val_loss: -0.8823 - val_negative_correlation_loss: -0.8863\nEpoch 14/50\n91/91 [==============================] - 5s 56ms/step - loss: -0.8980 - negative_correlation_loss: -0.9022 - val_loss: -0.8825 - val_negative_correlation_loss: -0.8864\n\nEpoch 00014: ReduceLROnPlateau reducing learning rate to 0.008999999798834325.\nEpoch 15/50\n91/91 [==============================] - 4s 47ms/step - loss: -0.8986 - negative_correlation_loss: -0.9025 - val_loss: -0.8842 - val_negative_correlation_loss: -0.8879\nEpoch 16/50\n91/91 [==============================] - 4s 48ms/step - loss: -0.8986 - negative_correlation_loss: -0.9026 - val_loss: -0.8809 - val_negative_correlation_loss: -0.8845\nEpoch 17/50\n91/91 [==============================] - 4s 47ms/step - loss: -0.8988 - negative_correlation_loss: -0.9028 - val_loss: -0.8798 - val_negative_correlation_loss: -0.8834\nEpoch 18/50\n91/91 [==============================] - 4s 47ms/step - loss: -0.8988 - negative_correlation_loss: -0.9027 - val_loss: -0.8828 - val_negative_correlation_loss: -0.8865\nEpoch 19/50\n91/91 [==============================] - 4s 49ms/step - loss: -0.8990 - negative_correlation_loss: -0.9028 - val_loss: -0.8839 - val_negative_correlation_loss: -0.8875\n\nEpoch 00019: ReduceLROnPlateau reducing learning rate to 0.008099999651312828.\nEpoch 20/50\n91/91 [==============================] - 4s 48ms/step - loss: -0.8993 - negative_correlation_loss: -0.9031 - val_loss: -0.8845 - val_negative_correlation_loss: -0.8880\nEpoch 21/50\n91/91 [==============================] - 5s 56ms/step - loss: -0.8995 - negative_correlation_loss: -0.9033 - val_loss: -0.8853 - val_negative_correlation_loss: -0.8887\nEpoch 22/50\n91/91 [==============================] - 4s 49ms/step - loss: -0.8994 - negative_correlation_loss: -0.9031 - val_loss: -0.8846 - val_negative_correlation_loss: -0.8882\nEpoch 23/50\n91/91 [==============================] - 5s 51ms/step - loss: -0.8997 - negative_correlation_loss: -0.9034 - val_loss: -0.8837 - val_negative_correlation_loss: -0.8870\nEpoch 24/50\n91/91 [==============================] - 4s 49ms/step - loss: -0.8996 - negative_correlation_loss: -0.9033 - val_loss: -0.8839 - val_negative_correlation_loss: -0.8875\nEpoch 25/50\n91/91 [==============================] - 4s 47ms/step - loss: -0.8995 - negative_correlation_loss: -0.9033 - val_loss: -0.8847 - val_negative_correlation_loss: -0.8882\n\nEpoch 00025: ReduceLROnPlateau reducing learning rate to 0.007289999350905419.\nEpoch 26/50\n91/91 [==============================] - 4s 48ms/step - loss: -0.9000 - negative_correlation_loss: -0.9036 - val_loss: -0.8853 - val_negative_correlation_loss: -0.8887\nEpoch 27/50\n91/91 [==============================] - 4s 47ms/step - loss: -0.9001 - negative_correlation_loss: -0.9037 - val_loss: -0.8850 - val_negative_correlation_loss: -0.8883\nEpoch 28/50\n91/91 [==============================] - 5s 57ms/step - loss: -0.9000 - negative_correlation_loss: -0.9036 - val_loss: -0.8858 - val_negative_correlation_loss: -0.8892\nEpoch 29/50\n91/91 [==============================] - 4s 47ms/step - loss: -0.9001 - negative_correlation_loss: -0.9038 - val_loss: -0.8850 - val_negative_correlation_loss: -0.8883\nEpoch 30/50\n91/91 [==============================] - 4s 48ms/step - loss: -0.8999 - negative_correlation_loss: -0.9036 - val_loss: -0.8848 - val_negative_correlation_loss: -0.8882\nEpoch 31/50\n91/91 [==============================] - 4s 48ms/step - loss: -0.9002 - negative_correlation_loss: -0.9038 - val_loss: -0.8845 - val_negative_correlation_loss: -0.8878\nEpoch 32/50\n91/91 [==============================] - 4s 48ms/step - loss: -0.9002 - negative_correlation_loss: -0.9038 - val_loss: -0.8860 - val_negative_correlation_loss: -0.8893\nEpoch 33/50\n91/91 [==============================] - 4s 48ms/step - loss: -0.9000 - negative_correlation_loss: -0.9037 - val_loss: -0.8832 - val_negative_correlation_loss: -0.8867\nEpoch 34/50\n91/91 [==============================] - 4s 47ms/step - loss: -0.9002 - negative_correlation_loss: -0.9037 - val_loss: -0.8850 - val_negative_correlation_loss: -0.8883\nEpoch 35/50\n91/91 [==============================] - 5s 56ms/step - loss: -0.9000 - negative_correlation_loss: -0.9037 - val_loss: -0.8844 - val_negative_correlation_loss: -0.8878\nEpoch 36/50\n91/91 [==============================] - 4s 48ms/step - loss: -0.9002 - negative_correlation_loss: -0.9037 - val_loss: -0.8847 - val_negative_correlation_loss: -0.8880\n\nEpoch 00036: ReduceLROnPlateau reducing learning rate to 0.006560999248176813.\nEpoch 37/50\n91/91 [==============================] - 4s 48ms/step - loss: -0.9005 - negative_correlation_loss: -0.9040 - val_loss: -0.8840 - val_negative_correlation_loss: -0.8873\nEpoch 38/50\n91/91 [==============================] - 4s 48ms/step - loss: -0.9007 - negative_correlation_loss: -0.9043 - val_loss: -0.8861 - val_negative_correlation_loss: -0.8893\nEpoch 39/50\n91/91 [==============================] - 4s 48ms/step - loss: -0.9006 - negative_correlation_loss: -0.9041 - val_loss: -0.8857 - val_negative_correlation_loss: -0.8889\nEpoch 40/50\n91/91 [==============================] - 4s 47ms/step - loss: -0.9008 - negative_correlation_loss: -0.9042 - val_loss: -0.8864 - val_negative_correlation_loss: -0.8896\nEpoch 41/50\n91/91 [==============================] - 4s 48ms/step - loss: -0.9006 - negative_correlation_loss: -0.9042 - val_loss: -0.8853 - val_negative_correlation_loss: -0.8885\nEpoch 42/50\n91/91 [==============================] - 5s 55ms/step - loss: -0.9008 - negative_correlation_loss: -0.9042 - val_loss: -0.8858 - val_negative_correlation_loss: -0.8889\nEpoch 43/50\n91/91 [==============================] - 4s 49ms/step - loss: -0.9003 - negative_correlation_loss: -0.9039 - val_loss: -0.8848 - val_negative_correlation_loss: -0.8881\nEpoch 44/50\n91/91 [==============================] - 4s 49ms/step - loss: -0.9006 - negative_correlation_loss: -0.9040 - val_loss: -0.8835 - val_negative_correlation_loss: -0.8867\n\nEpoch 00044: ReduceLROnPlateau reducing learning rate to 0.005904899490997195.\nEpoch 45/50\n91/91 [==============================] - 4s 48ms/step - loss: -0.9009 - negative_correlation_loss: -0.9042 - val_loss: -0.8863 - val_negative_correlation_loss: -0.8894\nEpoch 46/50\n91/91 [==============================] - 4s 49ms/step - loss: -0.9012 - negative_correlation_loss: -0.9045 - val_loss: -0.8856 - val_negative_correlation_loss: -0.8886\nEpoch 47/50\n91/91 [==============================] - 4s 48ms/step - loss: -0.9008 - negative_correlation_loss: -0.9041 - val_loss: -0.8854 - val_negative_correlation_loss: -0.8885\nEpoch 48/50\n91/91 [==============================] - 4s 49ms/step - loss: -0.9010 - negative_correlation_loss: -0.9043 - val_loss: -0.8844 - val_negative_correlation_loss: -0.8875\n\nEpoch 00048: ReduceLROnPlateau reducing learning rate to 0.00531440949998796.\nEpoch 49/50\n91/91 [==============================] - 5s 56ms/step - loss: -0.9013 - negative_correlation_loss: -0.9046 - val_loss: -0.8853 - val_negative_correlation_loss: -0.8883\nEpoch 50/50\n91/91 [==============================] - 4s 48ms/step - loss: -0.9014 - negative_correlation_loss: -0.9046 - val_loss: -0.8869 - val_negative_correlation_loss: -0.8899\n","output_type":"stream"},{"name":"stderr","text":"2022-11-14 04:09:50.448726: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n","output_type":"stream"},{"name":"stdout","text":"model saved\nFold 0, correlation =  0.89009\nEpoch 1/50\n92/92 [==============================] - 6s 51ms/step - loss: -0.7951 - negative_correlation_loss: -0.8126 - val_loss: -0.8408 - val_negative_correlation_loss: -0.8559\nEpoch 2/50\n92/92 [==============================] - 4s 47ms/step - loss: -0.8465 - negative_correlation_loss: -0.8582 - val_loss: -0.8564 - val_negative_correlation_loss: -0.8662\nEpoch 3/50\n92/92 [==============================] - 4s 48ms/step - loss: -0.8669 - negative_correlation_loss: -0.8756 - val_loss: -0.8729 - val_negative_correlation_loss: -0.8804\nEpoch 4/50\n92/92 [==============================] - 5s 56ms/step - loss: -0.8788 - negative_correlation_loss: -0.8857 - val_loss: -0.8783 - val_negative_correlation_loss: -0.8846\nEpoch 5/50\n92/92 [==============================] - 4s 48ms/step - loss: -0.8844 - negative_correlation_loss: -0.8903 - val_loss: -0.8826 - val_negative_correlation_loss: -0.8881\nEpoch 6/50\n92/92 [==============================] - 4s 48ms/step - loss: -0.8881 - negative_correlation_loss: -0.8935 - val_loss: -0.8849 - val_negative_correlation_loss: -0.8904\nEpoch 7/50\n92/92 [==============================] - 4s 48ms/step - loss: -0.8903 - negative_correlation_loss: -0.8956 - val_loss: -0.8860 - val_negative_correlation_loss: -0.8912\nEpoch 8/50\n92/92 [==============================] - 5s 50ms/step - loss: -0.8916 - negative_correlation_loss: -0.8967 - val_loss: -0.8865 - val_negative_correlation_loss: -0.8914\nEpoch 9/50\n92/92 [==============================] - 5s 49ms/step - loss: -0.8926 - negative_correlation_loss: -0.8974 - val_loss: -0.8862 - val_negative_correlation_loss: -0.8910\nEpoch 10/50\n92/92 [==============================] - 4s 49ms/step - loss: -0.8933 - negative_correlation_loss: -0.8980 - val_loss: -0.8876 - val_negative_correlation_loss: -0.8922\nEpoch 11/50\n92/92 [==============================] - 5s 56ms/step - loss: -0.8935 - negative_correlation_loss: -0.8981 - val_loss: -0.8877 - val_negative_correlation_loss: -0.8923\nEpoch 12/50\n92/92 [==============================] - 4s 47ms/step - loss: -0.8939 - negative_correlation_loss: -0.8985 - val_loss: -0.8879 - val_negative_correlation_loss: -0.8926\nEpoch 13/50\n92/92 [==============================] - 5s 49ms/step - loss: -0.8944 - negative_correlation_loss: -0.8988 - val_loss: -0.8886 - val_negative_correlation_loss: -0.8929\nEpoch 14/50\n92/92 [==============================] - 4s 48ms/step - loss: -0.8944 - negative_correlation_loss: -0.8988 - val_loss: -0.8884 - val_negative_correlation_loss: -0.8928\nEpoch 15/50\n92/92 [==============================] - 4s 49ms/step - loss: -0.8947 - negative_correlation_loss: -0.8990 - val_loss: -0.8863 - val_negative_correlation_loss: -0.8906\nEpoch 16/50\n92/92 [==============================] - 4s 48ms/step - loss: -0.8948 - negative_correlation_loss: -0.8991 - val_loss: -0.8893 - val_negative_correlation_loss: -0.8936\nEpoch 17/50\n92/92 [==============================] - 4s 47ms/step - loss: -0.8950 - negative_correlation_loss: -0.8993 - val_loss: -0.8892 - val_negative_correlation_loss: -0.8935\nEpoch 18/50\n92/92 [==============================] - 5s 57ms/step - loss: -0.8952 - negative_correlation_loss: -0.8995 - val_loss: -0.8894 - val_negative_correlation_loss: -0.8937\nEpoch 19/50\n92/92 [==============================] - 4s 48ms/step - loss: -0.8952 - negative_correlation_loss: -0.8995 - val_loss: -0.8892 - val_negative_correlation_loss: -0.8934\nEpoch 20/50\n92/92 [==============================] - 4s 49ms/step - loss: -0.8953 - negative_correlation_loss: -0.8995 - val_loss: -0.8884 - val_negative_correlation_loss: -0.8925\nEpoch 21/50\n92/92 [==============================] - 4s 47ms/step - loss: -0.8953 - negative_correlation_loss: -0.8996 - val_loss: -0.8883 - val_negative_correlation_loss: -0.8925\nEpoch 22/50\n92/92 [==============================] - 4s 48ms/step - loss: -0.8954 - negative_correlation_loss: -0.8997 - val_loss: -0.8876 - val_negative_correlation_loss: -0.8918\n\nEpoch 00022: ReduceLROnPlateau reducing learning rate to 0.008999999798834325.\nEpoch 23/50\n92/92 [==============================] - 4s 48ms/step - loss: -0.8958 - negative_correlation_loss: -0.8999 - val_loss: -0.8892 - val_negative_correlation_loss: -0.8932\nEpoch 24/50\n92/92 [==============================] - 4s 47ms/step - loss: -0.8959 - negative_correlation_loss: -0.8999 - val_loss: -0.8899 - val_negative_correlation_loss: -0.8940\nEpoch 25/50\n92/92 [==============================] - 5s 57ms/step - loss: -0.8961 - negative_correlation_loss: -0.9000 - val_loss: -0.8904 - val_negative_correlation_loss: -0.8943\nEpoch 26/50\n92/92 [==============================] - 4s 48ms/step - loss: -0.8961 - negative_correlation_loss: -0.9001 - val_loss: -0.8897 - val_negative_correlation_loss: -0.8937\nEpoch 27/50\n92/92 [==============================] - 4s 49ms/step - loss: -0.8961 - negative_correlation_loss: -0.9001 - val_loss: -0.8896 - val_negative_correlation_loss: -0.8936\nEpoch 28/50\n92/92 [==============================] - 4s 48ms/step - loss: -0.8960 - negative_correlation_loss: -0.9000 - val_loss: -0.8884 - val_negative_correlation_loss: -0.8924\nEpoch 29/50\n92/92 [==============================] - 4s 48ms/step - loss: -0.8960 - negative_correlation_loss: -0.9000 - val_loss: -0.8888 - val_negative_correlation_loss: -0.8927\n\nEpoch 00029: ReduceLROnPlateau reducing learning rate to 0.008099999651312828.\nEpoch 30/50\n92/92 [==============================] - 4s 48ms/step - loss: -0.8966 - negative_correlation_loss: -0.9005 - val_loss: -0.8908 - val_negative_correlation_loss: -0.8947\nEpoch 31/50\n92/92 [==============================] - 4s 47ms/step - loss: -0.8967 - negative_correlation_loss: -0.9005 - val_loss: -0.8900 - val_negative_correlation_loss: -0.8938\nEpoch 32/50\n92/92 [==============================] - 5s 57ms/step - loss: -0.8965 - negative_correlation_loss: -0.9003 - val_loss: -0.8894 - val_negative_correlation_loss: -0.8933\nEpoch 33/50\n92/92 [==============================] - 4s 48ms/step - loss: -0.8966 - negative_correlation_loss: -0.9004 - val_loss: -0.8899 - val_negative_correlation_loss: -0.8937\nEpoch 34/50\n92/92 [==============================] - 4s 47ms/step - loss: -0.8967 - negative_correlation_loss: -0.9005 - val_loss: -0.8909 - val_negative_correlation_loss: -0.8947\n\nEpoch 00034: ReduceLROnPlateau reducing learning rate to 0.007289999350905419.\nEpoch 35/50\n92/92 [==============================] - 4s 48ms/step - loss: -0.8970 - negative_correlation_loss: -0.9007 - val_loss: -0.8903 - val_negative_correlation_loss: -0.8940\nEpoch 36/50\n92/92 [==============================] - 4s 47ms/step - loss: -0.8970 - negative_correlation_loss: -0.9006 - val_loss: -0.8905 - val_negative_correlation_loss: -0.8942\nEpoch 37/50\n92/92 [==============================] - 4s 48ms/step - loss: -0.8970 - negative_correlation_loss: -0.9007 - val_loss: -0.8903 - val_negative_correlation_loss: -0.8940\nEpoch 38/50\n92/92 [==============================] - 4s 47ms/step - loss: -0.8971 - negative_correlation_loss: -0.9008 - val_loss: -0.8898 - val_negative_correlation_loss: -0.8934\n\nEpoch 00038: ReduceLROnPlateau reducing learning rate to 0.006560999248176813.\nEpoch 39/50\n92/92 [==============================] - 5s 56ms/step - loss: -0.8974 - negative_correlation_loss: -0.9010 - val_loss: -0.8909 - val_negative_correlation_loss: -0.8944\nEpoch 40/50\n92/92 [==============================] - 5s 50ms/step - loss: -0.8975 - negative_correlation_loss: -0.9010 - val_loss: -0.8902 - val_negative_correlation_loss: -0.8937\nEpoch 41/50\n92/92 [==============================] - 4s 48ms/step - loss: -0.8975 - negative_correlation_loss: -0.9010 - val_loss: -0.8892 - val_negative_correlation_loss: -0.8927\nEpoch 42/50\n92/92 [==============================] - 4s 49ms/step - loss: -0.8975 - negative_correlation_loss: -0.9011 - val_loss: -0.8905 - val_negative_correlation_loss: -0.8940\nEpoch 43/50\n92/92 [==============================] - 4s 47ms/step - loss: -0.8975 - negative_correlation_loss: -0.9010 - val_loss: -0.8903 - val_negative_correlation_loss: -0.8938\n\nEpoch 00043: ReduceLROnPlateau reducing learning rate to 0.005904899490997195.\nEpoch 44/50\n92/92 [==============================] - 4s 47ms/step - loss: -0.8978 - negative_correlation_loss: -0.9012 - val_loss: -0.8919 - val_negative_correlation_loss: -0.8953\nEpoch 45/50\n92/92 [==============================] - 4s 49ms/step - loss: -0.8980 - negative_correlation_loss: -0.9014 - val_loss: -0.8915 - val_negative_correlation_loss: -0.8948\nEpoch 46/50\n92/92 [==============================] - 5s 56ms/step - loss: -0.8979 - negative_correlation_loss: -0.9013 - val_loss: -0.8902 - val_negative_correlation_loss: -0.8937\nEpoch 47/50\n92/92 [==============================] - 4s 48ms/step - loss: -0.8979 - negative_correlation_loss: -0.9013 - val_loss: -0.8908 - val_negative_correlation_loss: -0.8943\nEpoch 48/50\n92/92 [==============================] - 4s 47ms/step - loss: -0.8979 - negative_correlation_loss: -0.9014 - val_loss: -0.8907 - val_negative_correlation_loss: -0.8941\n\nEpoch 00048: ReduceLROnPlateau reducing learning rate to 0.00531440949998796.\nEpoch 49/50\n92/92 [==============================] - 4s 48ms/step - loss: -0.8982 - negative_correlation_loss: -0.9016 - val_loss: -0.8909 - val_negative_correlation_loss: -0.8943\nEpoch 50/50\n92/92 [==============================] - 4s 47ms/step - loss: -0.8982 - negative_correlation_loss: -0.9015 - val_loss: -0.8920 - val_negative_correlation_loss: -0.8953\nmodel saved\nFold 1, correlation =  0.89528\nEpoch 1/50\n96/96 [==============================] - 7s 57ms/step - loss: -0.8036 - negative_correlation_loss: -0.8201 - val_loss: -0.8433 - val_negative_correlation_loss: -0.8559\nEpoch 2/50\n96/96 [==============================] - 5s 47ms/step - loss: -0.8643 - negative_correlation_loss: -0.8747 - val_loss: -0.8669 - val_negative_correlation_loss: -0.8753\nEpoch 3/50\n96/96 [==============================] - 4s 46ms/step - loss: -0.8785 - negative_correlation_loss: -0.8859 - val_loss: -0.8747 - val_negative_correlation_loss: -0.8811\nEpoch 4/50\n96/96 [==============================] - 4s 46ms/step - loss: -0.8858 - negative_correlation_loss: -0.8918 - val_loss: -0.8793 - val_negative_correlation_loss: -0.8848\nEpoch 5/50\n96/96 [==============================] - 5s 47ms/step - loss: -0.8891 - negative_correlation_loss: -0.8946 - val_loss: -0.8820 - val_negative_correlation_loss: -0.8870\nEpoch 6/50\n96/96 [==============================] - 4s 46ms/step - loss: -0.8911 - negative_correlation_loss: -0.8962 - val_loss: -0.8839 - val_negative_correlation_loss: -0.8888\nEpoch 7/50\n96/96 [==============================] - 6s 65ms/step - loss: -0.8921 - negative_correlation_loss: -0.8970 - val_loss: -0.8834 - val_negative_correlation_loss: -0.8879\nEpoch 8/50\n96/96 [==============================] - 5s 55ms/step - loss: -0.8931 - negative_correlation_loss: -0.8977 - val_loss: -0.8848 - val_negative_correlation_loss: -0.8891\nEpoch 9/50\n96/96 [==============================] - 5s 57ms/step - loss: -0.8932 - negative_correlation_loss: -0.8977 - val_loss: -0.8839 - val_negative_correlation_loss: -0.8884\nEpoch 10/50\n96/96 [==============================] - 4s 46ms/step - loss: -0.8938 - negative_correlation_loss: -0.8982 - val_loss: -0.8849 - val_negative_correlation_loss: -0.8890\nEpoch 11/50\n96/96 [==============================] - 4s 46ms/step - loss: -0.8943 - negative_correlation_loss: -0.8985 - val_loss: -0.8841 - val_negative_correlation_loss: -0.8882\nEpoch 12/50\n96/96 [==============================] - 4s 46ms/step - loss: -0.8944 - negative_correlation_loss: -0.8987 - val_loss: -0.8862 - val_negative_correlation_loss: -0.8903\nEpoch 13/50\n96/96 [==============================] - 4s 46ms/step - loss: -0.8947 - negative_correlation_loss: -0.8989 - val_loss: -0.8855 - val_negative_correlation_loss: -0.8896\nEpoch 14/50\n96/96 [==============================] - 5s 55ms/step - loss: -0.8948 - negative_correlation_loss: -0.8989 - val_loss: -0.8844 - val_negative_correlation_loss: -0.8885\nEpoch 15/50\n96/96 [==============================] - 4s 46ms/step - loss: -0.8950 - negative_correlation_loss: -0.8991 - val_loss: -0.8860 - val_negative_correlation_loss: -0.8899\nEpoch 16/50\n96/96 [==============================] - 4s 47ms/step - loss: -0.8950 - negative_correlation_loss: -0.8990 - val_loss: -0.8839 - val_negative_correlation_loss: -0.8879\n\nEpoch 00016: ReduceLROnPlateau reducing learning rate to 0.008999999798834325.\nEpoch 17/50\n96/96 [==============================] - 4s 47ms/step - loss: -0.8954 - negative_correlation_loss: -0.8994 - val_loss: -0.8869 - val_negative_correlation_loss: -0.8907\nEpoch 18/50\n96/96 [==============================] - 5s 48ms/step - loss: -0.8955 - negative_correlation_loss: -0.8994 - val_loss: -0.8859 - val_negative_correlation_loss: -0.8897\nEpoch 19/50\n96/96 [==============================] - 4s 47ms/step - loss: -0.8957 - negative_correlation_loss: -0.8996 - val_loss: -0.8865 - val_negative_correlation_loss: -0.8903\nEpoch 20/50\n96/96 [==============================] - 4s 46ms/step - loss: -0.8957 - negative_correlation_loss: -0.8996 - val_loss: -0.8862 - val_negative_correlation_loss: -0.8900\nEpoch 21/50\n96/96 [==============================] - 5s 54ms/step - loss: -0.8959 - negative_correlation_loss: -0.8998 - val_loss: -0.8862 - val_negative_correlation_loss: -0.8899\n\nEpoch 00021: ReduceLROnPlateau reducing learning rate to 0.008099999651312828.\nEpoch 22/50\n96/96 [==============================] - 4s 46ms/step - loss: -0.8963 - negative_correlation_loss: -0.9000 - val_loss: -0.8863 - val_negative_correlation_loss: -0.8899\nEpoch 23/50\n96/96 [==============================] - 4s 47ms/step - loss: -0.8965 - negative_correlation_loss: -0.9002 - val_loss: -0.8873 - val_negative_correlation_loss: -0.8908\nEpoch 24/50\n96/96 [==============================] - 4s 46ms/step - loss: -0.8965 - negative_correlation_loss: -0.9003 - val_loss: -0.8868 - val_negative_correlation_loss: -0.8904\nEpoch 25/50\n96/96 [==============================] - 4s 46ms/step - loss: -0.8964 - negative_correlation_loss: -0.9001 - val_loss: -0.8859 - val_negative_correlation_loss: -0.8895\nEpoch 26/50\n96/96 [==============================] - 4s 46ms/step - loss: -0.8966 - negative_correlation_loss: -0.9002 - val_loss: -0.8867 - val_negative_correlation_loss: -0.8903\nEpoch 27/50\n96/96 [==============================] - 4s 46ms/step - loss: -0.8965 - negative_correlation_loss: -0.9002 - val_loss: -0.8860 - val_negative_correlation_loss: -0.8895\n\nEpoch 00027: ReduceLROnPlateau reducing learning rate to 0.007289999350905419.\nEpoch 28/50\n96/96 [==============================] - 5s 55ms/step - loss: -0.8969 - negative_correlation_loss: -0.9004 - val_loss: -0.8877 - val_negative_correlation_loss: -0.8911\nEpoch 29/50\n96/96 [==============================] - 5s 48ms/step - loss: -0.8969 - negative_correlation_loss: -0.9004 - val_loss: -0.8863 - val_negative_correlation_loss: -0.8896\nEpoch 30/50\n96/96 [==============================] - 5s 48ms/step - loss: -0.8970 - negative_correlation_loss: -0.9005 - val_loss: -0.8865 - val_negative_correlation_loss: -0.8899\nEpoch 31/50\n96/96 [==============================] - 4s 47ms/step - loss: -0.8972 - negative_correlation_loss: -0.9006 - val_loss: -0.8867 - val_negative_correlation_loss: -0.8901\nEpoch 32/50\n96/96 [==============================] - 4s 46ms/step - loss: -0.8971 - negative_correlation_loss: -0.9006 - val_loss: -0.8868 - val_negative_correlation_loss: -0.8902\n\nEpoch 00032: ReduceLROnPlateau reducing learning rate to 0.006560999248176813.\nEpoch 33/50\n96/96 [==============================] - 4s 47ms/step - loss: -0.8973 - negative_correlation_loss: -0.9007 - val_loss: -0.8881 - val_negative_correlation_loss: -0.8914\nEpoch 34/50\n96/96 [==============================] - 4s 46ms/step - loss: -0.8972 - negative_correlation_loss: -0.9007 - val_loss: -0.8873 - val_negative_correlation_loss: -0.8906\nEpoch 35/50\n96/96 [==============================] - 5s 56ms/step - loss: -0.8973 - negative_correlation_loss: -0.9008 - val_loss: -0.8884 - val_negative_correlation_loss: -0.8917\nEpoch 36/50\n96/96 [==============================] - 4s 47ms/step - loss: -0.8974 - negative_correlation_loss: -0.9009 - val_loss: -0.8879 - val_negative_correlation_loss: -0.8912\nEpoch 37/50\n96/96 [==============================] - 4s 47ms/step - loss: -0.8974 - negative_correlation_loss: -0.9009 - val_loss: -0.8874 - val_negative_correlation_loss: -0.8906\nEpoch 38/50\n96/96 [==============================] - 4s 46ms/step - loss: -0.8974 - negative_correlation_loss: -0.9008 - val_loss: -0.8877 - val_negative_correlation_loss: -0.8911\nEpoch 39/50\n96/96 [==============================] - 4s 45ms/step - loss: -0.8975 - negative_correlation_loss: -0.9009 - val_loss: -0.8875 - val_negative_correlation_loss: -0.8909\n\nEpoch 00039: ReduceLROnPlateau reducing learning rate to 0.005904899490997195.\nEpoch 40/50\n96/96 [==============================] - 5s 47ms/step - loss: -0.8977 - negative_correlation_loss: -0.9011 - val_loss: -0.8872 - val_negative_correlation_loss: -0.8904\nEpoch 41/50\n96/96 [==============================] - 5s 54ms/step - loss: -0.8978 - negative_correlation_loss: -0.9011 - val_loss: -0.8865 - val_negative_correlation_loss: -0.8897\nEpoch 42/50\n96/96 [==============================] - 5s 48ms/step - loss: -0.8978 - negative_correlation_loss: -0.9010 - val_loss: -0.8883 - val_negative_correlation_loss: -0.8914\nEpoch 43/50\n96/96 [==============================] - 4s 47ms/step - loss: -0.8979 - negative_correlation_loss: -0.9012 - val_loss: -0.8884 - val_negative_correlation_loss: -0.8917\n\nEpoch 00043: ReduceLROnPlateau reducing learning rate to 0.00531440949998796.\nEpoch 44/50\n96/96 [==============================] - 4s 46ms/step - loss: -0.8981 - negative_correlation_loss: -0.9014 - val_loss: -0.8878 - val_negative_correlation_loss: -0.8908\nEpoch 45/50\n96/96 [==============================] - 5s 47ms/step - loss: -0.8981 - negative_correlation_loss: -0.9014 - val_loss: -0.8867 - val_negative_correlation_loss: -0.8898\nEpoch 46/50\n96/96 [==============================] - 4s 46ms/step - loss: -0.8983 - negative_correlation_loss: -0.9015 - val_loss: -0.8886 - val_negative_correlation_loss: -0.8917\nEpoch 47/50\n96/96 [==============================] - 4s 46ms/step - loss: -0.8982 - negative_correlation_loss: -0.9013 - val_loss: -0.8870 - val_negative_correlation_loss: -0.8902\nEpoch 48/50\n96/96 [==============================] - 5s 53ms/step - loss: -0.8982 - negative_correlation_loss: -0.9014 - val_loss: -0.8879 - val_negative_correlation_loss: -0.8909\nEpoch 49/50\n96/96 [==============================] - 4s 46ms/step - loss: -0.8983 - negative_correlation_loss: -0.9015 - val_loss: -0.8884 - val_negative_correlation_loss: -0.8915\nEpoch 50/50\n96/96 [==============================] - 4s 46ms/step - loss: -0.8984 - negative_correlation_loss: -0.9016 - val_loss: -0.8876 - val_negative_correlation_loss: -0.8907\n\nEpoch 00050: ReduceLROnPlateau reducing learning rate to 0.004782968759536744.\nmodel saved\nFold 2, correlation =  0.89174\n\u001b[32m\u001b[1mMean corr = 0.89237\u001b[0m\n\u001b[34m\u001b[1mOof corr   = 0.89236\u001b[0m\nCPU times: user 30min 31s, sys: 1min 8s, total: 31min 39s\nWall time: 11min 57s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Predictions for CITEseq","metadata":{}},{"cell_type":"code","source":"%%time\n# Around 20 s\n\ntest_pred = np.zeros((len(Xt), 140), dtype=np.float32)\nfor fold in range(N_SPLITS):\n    print(f\"Predicting with fold {fold}\")\n    model = load_model(f\"./submissions/model_{fold}\",\n                       custom_objects={'negative_correlation_loss': negative_correlation_loss})\n    test_pred += model.predict(Xt)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T04:18:00.005780Z","iopub.execute_input":"2022-11-14T04:18:00.006515Z","iopub.status.idle":"2022-11-14T04:18:14.325682Z","shell.execute_reply.started":"2022-11-14T04:18:00.006466Z","shell.execute_reply":"2022-11-14T04:18:14.324390Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Predicting with fold 0\nPredicting with fold 1\nPredicting with fold 2\nCPU times: user 19.9 s, sys: 2.51 s, total: 22.4 s\nWall time: 14.3 s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Save submission by merging with multiome","metadata":{}},{"cell_type":"code","source":"%%time\n# 2min 41s\n\n# Merge with multiome\nsubmission = pd.read_csv(multi_ome_only_file,index_col='row_id', squeeze=True)\nsubmission.iloc[:len(test_pred.ravel())] = test_pred.ravel()\nassert not submission.isna().any()\n\nsubmission.to_csv('submission_full.csv')\ndisplay(submission)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T04:18:14.327293Z","iopub.execute_input":"2022-11-14T04:18:14.328212Z","iopub.status.idle":"2022-11-14T04:21:41.273209Z","shell.execute_reply.started":"2022-11-14T04:18:14.328175Z","shell.execute_reply":"2022-11-14T04:21:41.271794Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"row_id\n0          -302.979309\n1          -289.225708\n2          -253.082932\n3           182.063492\n4           310.215454\n               ...    \n65744175      9.453125\n65744176      0.062561\n65744177      0.077026\n65744178      1.852539\n65744179      8.085938\nName: target, Length: 65744180, dtype: float64"},"metadata":{}},{"name":"stdout","text":"CPU times: user 3min 6s, sys: 9.35 s, total: 3min 15s\nWall time: 3min 26s\n","output_type":"stream"}]}]}
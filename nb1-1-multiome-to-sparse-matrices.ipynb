{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In notebook 1, several sparse matrices from this dataset by Fabien (https://www.kaggle.com/datasets/fabiencrom/multimodal-single-cell-as-sparse-matrix) were used\n\nFabien's notebook (https://www.kaggle.com/code/fabiencrom/multimodal-single-cell-creating-sparse-data/notebook) Explains how that dataset was obtained.\n\nFor completeness of explaining the full process from start to finish, this notebook will go over the creation of the sparse files which will be used in the later notebooks.\n\nNote: The files generated in these methods would be used in the multiome method from Xiafire (https://www.kaggle.com/code/xiafire/msci-multiome-5-steps-x-5-folds-25-models)","metadata":{}},{"cell_type":"markdown","source":"# First, all the basic imports and file names which may or may not be used is loaded in essentially as a headerÂ¶","metadata":{}},{"cell_type":"code","source":"import os, gc, pickle, datetime, scipy.sparse\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom colorama import Fore, Back, Style\n\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import TruncatedSVD,PCA\nfrom sklearn.metrics import mean_squared_error\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\nimport seaborn as sns\nfrom cycler import cycler\nfrom IPython.display import display\n\nimport scipy.sparse","metadata":{"execution":{"iopub.status.busy":"2022-11-13T21:41:02.766054Z","iopub.execute_input":"2022-11-13T21:41:02.766681Z","iopub.status.idle":"2022-11-13T21:41:02.776552Z","shell.execute_reply.started":"2022-11-13T21:41:02.766634Z","shell.execute_reply":"2022-11-13T21:41:02.775039Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Directory of the data\nDATA_DIR = \"/kaggle/input/open-problems-multimodal/\"\nFP_CELL_METADATA = os.path.join(DATA_DIR,\"metadata.csv\")\n\nFP_CITE_TRAIN_INPUTS = os.path.join(DATA_DIR,\"train_cite_inputs.h5\")\nFP_CITE_TRAIN_TARGETS = os.path.join(DATA_DIR,\"train_cite_targets.h5\")\nFP_CITE_TEST_INPUTS = os.path.join(DATA_DIR,\"test_cite_inputs.h5\")\n\nFP_MULT_TRAIN_INPUTS = os.path.join(DATA_DIR,\"train_multi_inputs.h5\")\nFP_MULT_TRAIN_TARGETS = os.path.join(DATA_DIR,\"train_multi_targets.h5\")\nFP_MULT_TEST_INPUTS = os.path.join(DATA_DIR,\"test_multi_inputs.h5\")\n\n# This notebook will go over how to make these files but...\n# FP_MULT_TRAIN_TARGETS_idx = \"./train_multi_targets_idxcol.npz\"\n# FP_MULT_TRAIN_TARGETS_sparse = \"./train_multi_targets_values.sparse.npz\"\n# FP_MULT_TRAIN_INPUTS_idx = \"./train_multi_inputs_idxcol.npz\"\n# FP_MULT_TRAIN_INPUTS_sparse = \"./train_multi_inputs_values.sparse.npz\"\n# FP_MULT_TEST_INPUTS_idx = \"./test_multi_inputs_idxcol.npz\"\n# FP_MULT_TEST_INPUTS_sparse = \"./test_multi_inputs_values.sparse.npz\"\n# Ultimately these files already exist so it would be a waste of space to\n# use the new created files. So, the files from https://www.kaggle.com/datasets/fabiencrom/multimodal-single-cell-as-sparse-matrix\n# will be used after going over how to make those files\nFP_MULT_TRAIN_TARGETS_idx = \"../input/multimodal-single-cell-as-sparse-matrix/train_multi_targets_idxcol.npz\"\nFP_MULT_TRAIN_TARGETS_sparse = \"../input/multimodal-single-cell-as-sparse-matrix/train_multi_targets_values.sparse.npz\"\nFP_MULT_TRAIN_INPUTS_idx = \"../input/multimodal-single-cell-as-sparse-matrix/train_multi_inputs_idxcol.npz\"\nFP_MULT_TRAIN_INPUTS_sparse = \"../input/multimodal-single-cell-as-sparse-matrix/train_multi_inputs_values.sparse.npz\"\nFP_MULT_TEST_INPUTS_idx = \"../input/multimodal-single-cell-as-sparse-matrix/test_multi_inputs_idxcol.npz\"\nFP_MULT_TEST_INPUTS_sparse = \"../input/multimodal-single-cell-as-sparse-matrix/test_multi_inputs_values.sparse.npz\"\n\nFP_SUBMISSION = os.path.join(DATA_DIR,\"sample_submission.csv\")\nFP_EVALUATION_IDS = os.path.join(DATA_DIR,\"evaluation_ids.csv\")\n\n# This file is made to go over the method but\n#FP_EVALUATION_IDS_parquet = \"./evaluation.parquet\"\n# Ultimately it's already saved here\nFP_EVALUATION_IDS_parquet = \"../input/multimodal-single-cell-as-sparse-matrix/evaluation.parquet\"\n\n# this is the original made file name\nmultiome_submission_filename = 'submission_multi_only.csv'\n\n# afterwards my session was terminated and so this needs to be loaded in order to\n# get the submission file:\nmultiome_submission_filename_saved = '../input/submission-based-on-xiafire/submission_multi_only.csv'\n\n# This is the new file which will be saved from the file above\n# (Some columns needed to be dropped)\nmultiome_submission_filename_cleaned_ver = 'submission_multi_only_cleaned.csv'\n\n# This is the saved name in case the kernel resets again\nmultiome_submission_filename_cleaned_ver_saved = '../input/cleaned-submission-based-on-xiafire/submission_multi_only_cleaned.csv'","metadata":{"execution":{"iopub.status.busy":"2022-11-13T21:41:02.779778Z","iopub.execute_input":"2022-11-13T21:41:02.780313Z","iopub.status.idle":"2022-11-13T21:41:02.796575Z","shell.execute_reply.started":"2022-11-13T21:41:02.780266Z","shell.execute_reply":"2022-11-13T21:41:02.795219Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Creating sparse data files for Multiome (only necessary for Multiome)","metadata":{}},{"cell_type":"markdown","source":"First, pytables is installed to deal with large amounts of data","metadata":{}},{"cell_type":"code","source":"%%time\n!conda install pytables -y","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-13T21:41:02.798057Z","iopub.execute_input":"2022-11-13T21:41:02.798418Z","iopub.status.idle":"2022-11-13T21:41:39.057633Z","shell.execute_reply.started":"2022-11-13T21:41:02.798387Z","shell.execute_reply":"2022-11-13T21:41:39.056036Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Collecting package metadata (current_repodata.json): done\nSolving environment: done\n\n# All requested packages already installed.\n\nRetrieving notices: ...working... done\nCPU times: user 962 ms, sys: 166 ms, total: 1.13 s\nWall time: 36.2 s\n","output_type":"stream"}]},{"cell_type":"code","source":"#necessary imports\nimport pandas as pd\nimport numpy as np\nimport scipy.sparse\nimport scipy","metadata":{"execution":{"iopub.status.busy":"2022-11-13T21:41:39.059362Z","iopub.execute_input":"2022-11-13T21:41:39.059738Z","iopub.status.idle":"2022-11-13T21:41:39.066997Z","shell.execute_reply.started":"2022-11-13T21:41:39.059705Z","shell.execute_reply":"2022-11-13T21:41:39.065605Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Sparse file creation function","metadata":{}},{"cell_type":"code","source":"# Function from Fabien's notebook\n\n# the multiome data is in h5 format\n# we want to convert this to sparse files\n\n# Inputs:\n# filename - original file name\n# out_filename - new file in sparse format\n# chunksize - how much is read at a time\n\n# Output:\n# No output, but result is that the two files\n# will be written. Once called out_filename_values.sparse.npz\n# and another called out_filename_idxcol.npz\n\n# The idea is to read in chunks (since it's not possible to read the full\n# file at once) and slowly save the file as a sparse matrix (which will\n# be able to be read all at once)\ndef convert_h5_to_sparse_csr(filename, out_filename, chunksize=2500):\n    start = 0\n    total_rows = 0\n\n    sparse_chunks_data_list = []\n    chunks_index_list = []\n    columns_name = None\n    while True:\n        df_chunk = pd.read_hdf(filename, start=start, stop=start+chunksize)\n        if len(df_chunk) == 0:\n            break\n        chunk_data_as_sparse = scipy.sparse.csr_matrix(df_chunk.to_numpy())\n        sparse_chunks_data_list.append(chunk_data_as_sparse)\n        chunks_index_list.append(df_chunk.index.to_numpy())\n\n        if columns_name is None:\n            columns_name = df_chunk.columns.to_numpy()\n        else:\n            assert np.all(columns_name == df_chunk.columns.to_numpy())\n\n        total_rows += len(df_chunk)\n        print(total_rows)\n        if len(df_chunk) < chunksize: \n            del df_chunk\n            break\n        del df_chunk\n        start += chunksize\n        \n    all_data_sparse = scipy.sparse.vstack(sparse_chunks_data_list)\n    del sparse_chunks_data_list\n    \n    all_indices = np.hstack(chunks_index_list)\n    \n    scipy.sparse.save_npz(out_filename+\"_values.sparse\", all_data_sparse)\n    np.savez(out_filename+\"_idxcol.npz\", index=all_indices, columns =columns_name)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-11-13T21:41:39.072887Z","iopub.execute_input":"2022-11-13T21:41:39.074265Z","iopub.status.idle":"2022-11-13T21:41:39.091965Z","shell.execute_reply.started":"2022-11-13T21:41:39.074221Z","shell.execute_reply":"2022-11-13T21:41:39.090221Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Using the function","metadata":{}},{"cell_type":"code","source":"%%time\n# Approximately 7 minutes\n\nconvert_h5_to_sparse_csr(FP_MULT_TRAIN_TARGETS, \n                         \"train_multi_targets\")\n\n# This gives:\n# FP_MULT_TRAIN_TARGETS_idx = \"./train_multi_targets_idxcol.npz\"\n# FP_MULT_TRAIN_TARGETS_sparse = \"./train_multi_targets_values.sparse.npz\"","metadata":{"execution":{"iopub.status.busy":"2022-11-13T21:41:39.093695Z","iopub.execute_input":"2022-11-13T21:41:39.094298Z","iopub.status.idle":"2022-11-13T21:48:04.338177Z","shell.execute_reply.started":"2022-11-13T21:41:39.094240Z","shell.execute_reply":"2022-11-13T21:48:04.336611Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\n22500\n25000\n27500\n30000\n32500\n35000\n37500\n40000\n42500\n45000\n47500\n50000\n52500\n55000\n57500\n60000\n62500\n65000\n67500\n70000\n72500\n75000\n77500\n80000\n82500\n85000\n87500\n90000\n92500\n95000\n97500\n100000\n102500\n105000\n105942\nCPU times: user 5min 44s, sys: 15 s, total: 5min 59s\nWall time: 6min 25s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n# Approximately 22 minutes\n\nconvert_h5_to_sparse_csr(FP_MULT_TRAIN_INPUTS, \n                         \"train_multi_inputs\")\n\n# This gives:\n# FP_MULT_TRAIN_INPUTS_idx = \"./train_multi_inputs_idxcol.npz\"\n# FP_MULT_TRAIN_INPUTS_sparse = \"./train_multi_inputs_values.sparse.npz\"","metadata":{"execution":{"iopub.status.busy":"2022-11-13T21:48:04.339898Z","iopub.execute_input":"2022-11-13T21:48:04.340773Z","iopub.status.idle":"2022-11-13T22:14:16.940241Z","shell.execute_reply.started":"2022-11-13T21:48:04.340722Z","shell.execute_reply":"2022-11-13T22:14:16.937790Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\n22500\n25000\n27500\n30000\n32500\n35000\n37500\n40000\n42500\n45000\n47500\n50000\n52500\n55000\n57500\n60000\n62500\n65000\n67500\n70000\n72500\n75000\n77500\n80000\n82500\n85000\n87500\n90000\n92500\n95000\n97500\n100000\n102500\n105000\n105942\nCPU times: user 23min 7s, sys: 1min 17s, total: 24min 24s\nWall time: 26min 12s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n# Approximately 13 minutes\nconvert_h5_to_sparse_csr(FP_MULT_TEST_INPUTS, \n                         \"test_multi_inputs\")\n\n# This gives:\n# FP_MULT_TEST_INPUTS_idx = \"./test_multi_inputs_idxcol.npz\"\n# FP_MULT_TEST_INPUTS_sparse = \"./test_multi_inputs_values.sparse.npz\"","metadata":{"execution":{"iopub.status.busy":"2022-11-13T22:14:16.943234Z","iopub.execute_input":"2022-11-13T22:14:16.943699Z","iopub.status.idle":"2022-11-13T22:29:19.077913Z","shell.execute_reply.started":"2022-11-13T22:14:16.943664Z","shell.execute_reply":"2022-11-13T22:29:19.076237Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\n22500\n25000\n27500\n30000\n32500\n35000\n37500\n40000\n42500\n45000\n47500\n50000\n52500\n55000\n55935\nCPU times: user 12min 38s, sys: 41.3 s, total: 13min 19s\nWall time: 15min 2s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Some other files were also converted to parquet format for efficiency","metadata":{}},{"cell_type":"code","source":"def convert_to_parquet(filename, out_filename):\n    df = pd.read_csv(filename)\n    df.to_parquet(out_filename + \".parquet\")","metadata":{"execution":{"iopub.status.busy":"2022-11-13T22:29:19.079732Z","iopub.execute_input":"2022-11-13T22:29:19.080147Z","iopub.status.idle":"2022-11-13T22:29:19.087489Z","shell.execute_reply.started":"2022-11-13T22:29:19.080111Z","shell.execute_reply":"2022-11-13T22:29:19.086134Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"convert_to_parquet(FP_EVALUATION_IDS, \"evaluation\")\n# This creates FP_EVALUATION_IDS_parquet = \"./evaluation.parquet\"","metadata":{"execution":{"iopub.status.busy":"2022-11-13T22:29:19.089454Z","iopub.execute_input":"2022-11-13T22:29:19.089849Z","iopub.status.idle":"2022-11-13T22:30:45.443141Z","shell.execute_reply.started":"2022-11-13T22:29:19.089817Z","shell.execute_reply":"2022-11-13T22:30:45.441333Z"},"trusted":true},"execution_count":15,"outputs":[]}]}